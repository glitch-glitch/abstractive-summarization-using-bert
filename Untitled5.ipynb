{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d278aa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.24.0\n",
      "tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast, TFEncoderDecoderModel, AdamWeightDecay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from pprint import pprint\n",
    "\"\"\n",
    "# enable model saving at eager mode\n",
    "np_config.enable_numpy_behavior()\n",
    "print('transformers version:', transformers.__version__)\n",
    "print('tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b74ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill==0.3.6\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Installing collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.2.8\n",
      "    Uninstalling dill-0.2.8:\n",
      "      Successfully uninstalled dill-0.2.8\n",
      "Successfully installed dill-0.3.6\n"
     ]
    }
   ],
   "source": [
    "! pip install dill==0.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca497873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "421cf6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59e2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.drop_duplicates(subset=['UserId','Text'], inplace=True)\n",
    "df_reviews.dropna(subset=['Summary'], inplace=True)\n",
    "df_reviews.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a60e32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.to_pickle('df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b84a38bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of length of tokenized summary')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_reviews = pd.read_pickle('df.pkl')\n",
    "df_reviews_sampled = df_reviews.copy()\n",
    "# sampling n pairs of reviews and summarizies on shuffled dataframe\n",
    "df_reviews_sampled = df_reviews_sampled.iloc[:40000, :]\n",
    "df_reviews_sampled['text_len'] = df_reviews_sampled['Text'].apply(lambda x: len(x.split()))\n",
    "df_reviews_sampled['summary_len'] = df_reviews_sampled['Summary'].apply(lambda x: len(x.split()))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\n",
    "ax[0].hist(df_reviews_sampled['text_len'])\n",
    "ax[0].set_xticks(np.arange(0, max(df_reviews_sampled['text_len'])+1,250))\n",
    "ax[0].set_title('Histogram of length of tokenized text')\n",
    "ax[1].hist(df_reviews_sampled['summary_len'], color='yellow')\n",
    "ax[1].set_xticks(np.arange(0, max(df_reviews_sampled['summary_len'])+1,5))\n",
    "ax[1].set_title('Histogram of length of tokenized summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "122387d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    val_split = 0.2\n",
    "    pretrained_checkpoint = 'bert-base-uncased'\n",
    "    encoder_checkpoint = 'bert-base-uncased'\n",
    "    decoder_checkpoint = 'bert-base-uncased'\n",
    "    pad_token_id = 0\n",
    "    shared_weight = False\n",
    "    encoder_max_len = 256 \n",
    "    decoder_max_len = 30 \n",
    "    nb_epoch = 3 \n",
    "    learning_rate = 3e-5 \n",
    "    batch_size = 8 \n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9340a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, paragraphs, summaries, **kwargs):\n",
    "        self.paragraphs = paragraphs \n",
    "        self.summaries = summaries \n",
    "        self.tokenizer = kwargs.get('tokenizer')\n",
    "        self.val_split = kwargs.get('val_split')\n",
    "        self.encoder_max_len = kwargs.get('encoder_max_len')\n",
    "        self.decoder_max_len = kwargs.get('decoder_max_len')\n",
    "    \n",
    "    @property\n",
    "    def sample_size(self):\n",
    "        assert len(self.paragraphs)==len(self.summaries)\n",
    "        return len(self.paragraphs)\n",
    "    \n",
    "    def split_train_test(self):\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            list(range(self.sample_size)), \n",
    "            test_size=self.val_split, \n",
    "            random_state=98\n",
    "        )\n",
    "        return train_idx, val_idx\n",
    "    \n",
    "    def convert_text_to_ids(self, input_paragraphs, input_summaries):\n",
    "        inputs = self.tokenizer(\n",
    "            list(input_paragraphs), \n",
    "            return_tensors='np', \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.encoder_max_len\n",
    "        )\n",
    "        outputs = self.tokenizer(\n",
    "            list(input_summaries), \n",
    "            return_tensors='np', \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.decoder_max_len\n",
    "        )\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def list_to_tensor_dataset(self, input_paragraphs, input_summaries):\n",
    "        inputs, outputs = self.convert_text_to_ids(\n",
    "            input_paragraphs, \n",
    "            input_summaries\n",
    "        )\n",
    "        input_ids = tf.data.Dataset.from_tensor_slices(\n",
    "            inputs['input_ids']\n",
    "        )\n",
    "        attention_masks = tf.data.Dataset.from_tensor_slices(\n",
    "            inputs['attention_mask']\n",
    "        )\n",
    "        output_ids = tf.data.Dataset.from_tensor_slices(\n",
    "            outputs['input_ids']\n",
    "        )\n",
    "        output_attention_masks = tf.data.Dataset.from_tensor_slices(\n",
    "            outputs['attention_mask']\n",
    "        )                                                \n",
    "        tf_dataset = tf.data.Dataset.zip(\n",
    "            ({\n",
    "                'input_ids': input_ids, \n",
    "                'attention_mask': attention_masks,\n",
    "                'decoder_input_ids': output_ids, \n",
    "                 'decoder_attention_mask': output_attention_masks\n",
    "            }, \n",
    "            output_ids)\n",
    "        )\n",
    "        return tf_dataset\n",
    "    \n",
    "    def __call__(self):\n",
    "        train_idx, val_idx = self.split_train_test()\n",
    "        train_paras, val_paras = self.paragraphs[train_idx], self.paragraphs[val_idx]\n",
    "        train_sums, val_sums = self.summaries[train_idx], self.summaries[val_idx]\n",
    "        train_dataset = self.list_to_tensor_dataset(train_paras, train_sums)\n",
    "        val_dataset = self.list_to_tensor_dataset(val_paras, val_sums)\n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95d852a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, pad_token_id, name=\"seq2seq_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, \n",
    "            reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        # shift the label and output sequences to match  \n",
    "        output_logits = y_pred[:,:-1,:]\n",
    "        input_labels = y_true[:,1:] \n",
    "        loss = loss_fn(input_labels, output_logits)\n",
    "        # calculate loss without the padding tokens in label sequence\n",
    "        mask = tf.cast((input_labels != self.pad_token_id), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b4ad85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, metric):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.metric = metric\n",
    "        # loss tracker will capture the mean of loss till now\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='mean_loss')\n",
    "    \n",
    "    # Training Step\n",
    "    @tf.function \n",
    "    def train_step(self, inputs):\n",
    "        input_seqs, input_labels = inputs\n",
    "        with tf.GradientTape() as tape: \n",
    "            outputs = self.model(\n",
    "                input_seqs['input_ids'],\n",
    "                input_seqs['attention_mask'],\n",
    "                input_seqs['decoder_input_ids'],\n",
    "                input_seqs['decoder_attention_mask'],\n",
    "                training = True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = self.loss_fn(input_labels, logits)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.model.trainable_weights)\n",
    "        )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "#         self.metric.update_state(y, predictions)\n",
    "        return loss\n",
    "        \n",
    "    # Validation Step\n",
    "    @tf.function  \n",
    "    def val_step(self, inputs):\n",
    "        input_seqs, input_labels = inputs\n",
    "        outputs = self.model(                \n",
    "                input_seqs['input_ids'],\n",
    "                input_seqs['attention_mask'],\n",
    "                input_seqs['decoder_input_ids'],\n",
    "                input_seqs['decoder_attention_mask'],\n",
    "                training = False\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fn(input_labels, logits)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "#         self.metric.update_state(y,predictions)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "593df379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_generate_summary(model, tokenizer, batched_input):\n",
    "    input_seqs, input_labels = batched_input\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_seqs['input_ids'], \n",
    "        attention_mask=input_seqs['attention_mask']\n",
    "    )\n",
    "    output_strs = tokenizer.batch_decode(\n",
    "        outputs, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    output_gold = tokenizer.batch_decode(\n",
    "        input_seqs['decoder_input_ids'], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    input_strs = tokenizer.batch_decode(\n",
    "        input_seqs['input_ids'], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return output_strs, output_gold, input_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c207ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d597e3ad37b84bc5af83d1199d09f2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\utkar_0x0wy73\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b906df4cac444c6a75b0f13901e9dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e159f47324b4d8a7abe7d3f31bc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34b423c2ec048318e57828289aba6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews = df_reviews_sampled['Text'].values\n",
    "summaries = df_reviews_sampled['Summary'].values\n",
    "\n",
    "training_config = TrainingConfig(nb_epoch=5)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(training_config.encoder_checkpoint)\n",
    "\n",
    "dataloader_args = {\n",
    "    'tokenizer': tokenizer,\n",
    "    'val_split': training_config.val_split,\n",
    "    'encoder_max_len': training_config.encoder_max_len,\n",
    "    'decoder_max_len': training_config.decoder_max_len\n",
    "}\n",
    "dataloader = DataLoader(reviews, summaries, **dataloader_args)\n",
    "train_dataset, val_dataset = dataloader()\n",
    "train_dataset = (train_dataset\n",
    "                 .shuffle(int(dataloader.sample_size*(1-training_config.val_split)))\n",
    "                 .batch(training_config.batch_size))\n",
    "val_dataset = val_dataset.batch(training_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4d4ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e366abe0a8b4542b8071cbec0bb8f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
      "\n",
      "Some layers of TFBertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._3/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._4/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/self/query/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/value/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/value/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._7/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/output/LayerNorm/beta:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._11/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._5/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._9/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/output/LayerNorm/gamma:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/key/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._0/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._6/crossattention/output/dense/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._8/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/self/query/bias:0', 'tf_encoder_decoder_model/bert/encoder/layer_._1/crossattention/self/key/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._10/crossattention/output/dense/kernel:0', 'tf_encoder_decoder_model/bert/encoder/layer_._2/crossattention/output/LayerNorm/beta:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert2bert = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    training_config.encoder_checkpoint, \n",
    "    training_config.decoder_checkpoint,\n",
    "    tie_encoder_decoder=training_config.shared_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55bf8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFEncoderDecoderModel.\n",
      "\n",
      "All the layers of TFEncoderDecoderModel were initialized from the model checkpoint at bert2bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFEncoderDecoderModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert2bert.save_pretrained('bert2bert')\n",
    "bert2bert = TFEncoderDecoderModel.from_pretrained('bert2bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c414a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id # 101\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id # 102 \n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id # 0\n",
    "bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config.max_length = 30\n",
    "bert2bert.config.min_length = 3\n",
    "bert2bert.config.no_repeat_ngram_size = 2\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60898462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge = datasets.load_metric('rouge')\n",
    "tf.keras.backend.clear_session()\n",
    "trainer = Trainer(model=bert2bert,\n",
    "                  loss_fn=Seq2SeqLoss(training_config.pad_token_id),\n",
    "                  optimizer=AdamWeightDecay(\n",
    "                      learning_rate=training_config.learning_rate, \n",
    "                      weight_decay_rate=0.005\n",
    "                  ),\n",
    "                  metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f620988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: dill in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from tqdm>=4.27->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkar_0x0wy73\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e83bd1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "\n",
      "Training....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4204e8195acd4434a17d67d486a40609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_encoder_decoder_model_1/encoder/bert/pooler/dense/kernel:0', 'tf_encoder_decoder_model_1/encoder/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_encoder_decoder_model_1/encoder/bert/pooler/dense/kernel:0', 'tf_encoder_decoder_model_1/encoder/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Training loss for one batch at step 0: 10.633999824523926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining....\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step,batched_input \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataset)):\n\u001b[1;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     till_now_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mloss_tracker\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_config.nb_epoch):\n",
    "    print(f'\\nEpoch {epoch+1}\\n')\n",
    "    print('Training....')\n",
    "    for step,batched_input in enumerate(tqdm(train_dataset)):\n",
    "        loss = trainer.train_step(batched_input)\n",
    "        till_now_loss = trainer.loss_tracker.result()\n",
    "        if step%200 == 0:\n",
    "            print(f'Training loss for one batch at step {step}: {round(till_now_loss,3)}') \n",
    "    trainer.loss_tracker.reset_states()\n",
    "     print('Validating....')\n",
    "    val_measures = {'rouge precision':0, 'rouge recall':0, 'rouge f1': 0}\n",
    "    for step, batched_input in enumerate(tqdm(val_dataset)):\n",
    "        val_loss = trainer.val_step(batched_input)\n",
    "#         pred_str, gold_str = generate_summary(bert2bert, \n",
    "#                                               tokenizer, \n",
    "#                                               batched_input)\n",
    "#         rouge_output = rouge.compute(predictions=pred_str,\n",
    "#                                      references=gold_str,\n",
    "#                                      rouge_types=['rouge2'])['rouge2'].mid\n",
    "#         val_measures['rouge precision'] += rouge_output.precision / len(val_dataset)\n",
    "#         val_measures['rouge recall'] += rouge_output.recall / len(val_dataset)\n",
    "#         val_measures['rouge f1'] += rouge_output.fmeasure / len(val_dataset)\n",
    "    till_now_val_loss = trainer.loss_tracker.result()\n",
    "    print(f'Validation loss: {round(till_now_val_loss,3)}')\n",
    "    bert2bert.save_pretrained(\n",
    "        f'bert2bert-Checkpoint-epoch{epoch+1}-loss{round(till_now_val_loss,3)}'\n",
    "    )\n",
    "#     for name, value in val_measures.items():\n",
    "#         print(f'Validation {name}: {value}')\n",
    "    trainer.loss_tracker.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val0 = list(val_dataset.as_numpy_iterator())[0]\n",
    "for step, batched_input in enumerate(tqdm(val_dataset)):\n",
    "    pred_str, gold_str, input_strs = batched_generate_summary(\n",
    "        trained_bert2bert, \n",
    "        tokenizer, \n",
    "        batched_input\n",
    "    )\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str,\n",
    "        references=gold_str,\n",
    "        rouge_types=[\"rouge1\"]\n",
    "    )\n",
    "    print('Rouge report: ')\n",
    "    print(rouge_output['rouge1'].mid)\n",
    "    for p_str,g_str,in_str in zip(pred_str, gold_str, input_strs):\n",
    "        print('='*100)\n",
    "        print('Review: ' + in_str)\n",
    "        print('Summary: ' + g_str)\n",
    "        print('Generated: ' + p_str)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_test = df_reviews.copy()\n",
    "df_reviews_test = df_reviews_test.iloc[40000:41000, :]\n",
    "test_reviews = df_reviews_test['Text'].values\n",
    "test_sums = df_reviews_test['Summary'].values\n",
    "\n",
    "test_dataloader = DataLoader(test_reviews, test_sums, **dataloader_args)\n",
    "test_dataset = test_dataloader.list_to_tensor_dataset(test_paras, test_sums)\n",
    "test_dataset = test_dataset.batch(training_config.batch_size)\n",
    "pred_strs = []\n",
    "gold_strs = []\n",
    "\n",
    "for batched_input in tqdm(test_dataset):\n",
    "    pred_str, gold_str, _ = batched_generate_summary(\n",
    "        trained_bert2bert, \n",
    "        tokenizer, \n",
    "        batched_input\n",
    "    )\n",
    "    pred_strs.extend(pred_str)\n",
    "    gold_strs.extend(gold_str)\n",
    "    \n",
    "rouge_output = rouge.compute(\n",
    "    predictions=pred_strs,\n",
    "    references=gold_strs,\n",
    "    rouge_types=[\"rouge1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff03cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bf046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ea8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b8993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c1d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2791073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8868709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
